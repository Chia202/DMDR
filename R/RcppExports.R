# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @name dmdrDense
#' @title Distributed Mean Dimension Reduction with Dense Solutions
#' @description Implements distributed parameter estimation for dimension reduction using data partitioned across multiple machines.
#' The algorithm iteratively updates parameters by computing local scores and Hessian matrices, combining these to refine global estimates.
#'
#' @param data A numeric matrix where the last column represents the response variable, and preceding columns contain predictor variables.
#' @param machines An integer specifying the number of machines to distribute computation across.
#' @param d An integer indicating the target reduced dimension for parameter estimation.
#' @param maxIter An integer specifying the maximum number of iterations (default: 100).
#' @param tol A numeric value representing the convergence tolerance for parameter updates (default: 1e-6).
#' @param useParallel A logical flag indicating whether to use parallel computation (default: FALSE).
#'
#' @return A numeric matrix containing the final estimated reduced-rank coefficient matrix.
#'
#' @details
#' This function partitions the data across the specified number of machines, performs local computations to estimate
#' scores and Hessian matrices, and aggregates these to iteratively refine global parameter estimates. The bandwidth parameters
#' \eqn{h_1}, \eqn{h_2}, and \eqn{h_3} are dynamically adjusted based on the partition size \eqn{n} as \eqn{n^{-1 / (4 + d)}}.
#' The algorithm stops iterating when the maximum change in parameters falls below the specified `tol`.
#' Singular Value Decomposition (SVD) is applied to the final coefficient matrix to enhance stability and interpretability.
#'
#' @export
dmdrDense <- function(data, machines, d, maxIter = 100L, tol = 1e-6, useParallel = FALSE) {
    .Call('_DMDR_dmdrDense', PACKAGE = 'DMDR', data, machines, d, maxIter, tol, useParallel)
}

#' @name dmdrSparse
#' @title Distributed Mean Dimension Reduction with Sparse Solutions
#' @description Performs distributed parameter estimation for dimension reduction while enforcing sparsity in the solution.
#' This method is designed for large-scale data partitioned across multiple machines and incorporates regularization for sparse outputs.
#'
#' @param data A numeric matrix where the last column represents the response variable, and preceding columns contain predictor variables.
#' @param machines An integer specifying the number of machines to distribute computation across.
#' @param d An integer indicating the target reduced dimension for parameter estimation.
#' @param maxIter An integer specifying the maximum number of iterations (default: 100).
#' @param tol A numeric value representing the convergence tolerance for parameter updates (default: 1e-6).
#' @param useParallel A logical flag indicating whether to use parallel computation (default: FALSE).
#'
#' @return A numeric matrix containing the final estimated sparse coefficient matrix.
#'
#' @details
#' The `dmdrSparse` function partitions the input data across the specified number of machines and performs local computations
#' to estimate sparse scores and Hessian matrices. Regularization is applied to enforce sparsity, making this approach suitable
#' for high-dimensional data where many predictor variables may not contribute significantly to the response.
#'
#' The algorithm iteratively aggregates local computations to refine global parameter estimates. Convergence is determined
#' when the maximum parameter update falls below the specified tolerance (`tol`). The final sparse solution enhances model
#' interpretability and reduces computational overhead. Parallel computation can be enabled with the `useParallel` flag for
#' faster execution on multi-core systems.
#'
#' @export
dmdrSparse <- function(data, machines, d, maxIter = 100L, tol = 1e-6, useParallel = FALSE) {
    .Call('_DMDR_dmdrSparse', PACKAGE = 'DMDR', data, machines, d, maxIter, tol, useParallel)
}

#' @name solveLasso
#' @title Solve Lasso Regression Using Coordinate Descent
#' @description Implements the Lasso (Least Absolute Shrinkage and Selection Operator) regression using the coordinate descent algorithm. It solves for sparse regression coefficients by penalizing the \(\ell_1\)-norm of the coefficients.
#'
#' @param H1 A numeric matrix (p x p) representing the covariance matrix or a related positive-definite matrix derived from the predictors. A small diagonal perturbation is added to ensure numerical stability.
#' @param betaCoef A numeric vector (p) representing the linear coefficients or gradient vector, typically derived from the regression problem.
#' @param lambda A numeric value specifying the regularization parameter. Higher values of \code{lambda} enforce greater sparsity by shrinking more coefficients to zero (default: 0.1).
#' @param maxIter An integer specifying the maximum number of iterations for the coordinate descent algorithm (default: 100).
#' @param tol A numeric value specifying the convergence tolerance. The algorithm terminates when the change in coefficients between iterations falls below this threshold (default: 1e-6).
#'
#' @return A numeric vector (p) of estimated regression coefficients. Sparse solutions are achieved due to the \(\ell_1\)-penalty applied by the Lasso method.
#'
#' @details
#' The `solveLasso` function uses the coordinate descent algorithm to optimize the Lasso objective function:
#' \deqn{\frac{1}{2} \mathbf{x}^T H_1 \mathbf{x} + \mathbf{\beta}^T \mathbf{x} + \lambda \|\mathbf{x}\|_1,}
#' where \(\|\mathbf{x}\|_1\) is the \(\ell_1\)-norm of \(\mathbf{x}\), encouraging sparsity in the solution. 
#'
#' Each iteration updates one coefficient at a time while keeping others fixed. The update uses a soft-thresholding operator to shrink coefficients based on the penalty parameter \code{lambda}. The algorithm iteratively refines the coefficients until convergence is achieved or the maximum number of iterations is reached.
#'
#' To ensure numerical stability, a small diagonal perturbation (e.g., \(10^{-3}\)) is added to \code{H1}, forming the matrix \code{H} internally. This helps prevent singularity or poorly conditioned matrices from affecting convergence.
#'
#' @examples
#' \dontrun{
#' H1 <- matrix(c(4, 1, 1, 3), nrow = 2)
#' betaCoef <- c(-1, 2)
#' lambda <- 0.5
#' result <- solveLasso(H1, betaCoef, lambda)
#' print(result)  # Sparse solution with \ell_1 regularization
#' }
#' @export
solveLasso <- function(H1, betaCoef, lambda = 0.1, maxIter = 100L, tol = 1e-6) {
    .Call('_DMDR_solveLasso', PACKAGE = 'DMDR', H1, betaCoef, lambda, maxIter, tol)
}

#' @name epanKernel
#' @title Epanichnikov Kernel for One Dimension
#' @description Computes the Epanichnikov kernel value for a scalar input.
#' @param u A numeric value representing the input.
#' @return A numeric value representing the kernel evaluation.
#' @export
NULL

#' @name epanKernelMulti
#' @title Multivariate Epanichnikov Kernel
#' @description Computes the product of Epanichnikov kernel values for a multivariate input.
#' @param u A numeric row vector representing the input.
#' @return A numeric value representing the kernel evaluation.
#' @export
NULL

#' @name gaussianKernel
#' @title Gaussian Kernel
#' @description This function computes the Gaussian kernel value for a given input vector. The kernel is applied element-wise to the input vector `u` and returns the product of the Gaussian functions of each element.
#' @param u A numeric row vector (p) representing the input data. Each element of this vector is used to calculate the corresponding Gaussian kernel value.
#' @return A numeric value representing the result of the Gaussian kernel function applied to the input vector `u`. The value is calculated as the product of Gaussian functions applied element-wise to each component of `u`.
#'   If the result is less than 0.01, it returns 0.01 to avoid extremely small values that might cause numerical instability in further computations.
#' @export
NULL

#' @name vecLower
#' @title Extract Lower Triangular Block of a Matrix
#' @description Extracts and vectorizes the lower triangular portion of a matrix.
#' @param m A numeric matrix input.
#' @param p Number of rows in the matrix.
#' @param d Number of columns in the triangular block.
#' @return A numeric vector representing the vectorized lower triangular block.
#' @export
NULL

#' @name vecInv
#' @title Inverse Operation of vecLower
#' @description Reconstructs a matrix from its vectorized lower triangular block.
#' @param v A numeric vector representing the lower triangular block.
#' @param p Number of rows in the reconstructed matrix.
#' @param d Number of columns in the triangular block.
#' @return A numeric matrix reconstructed from the input vector.
#' @export
NULL

#' @name cmptWeights
#' @title Compute Kernel-Based Weights
#' @description This function calculates observation weights using a kernel density estimation approach.
#'              The weights reflect the kernel-weighted variance of residuals for each observation.
#' @param X A numeric matrix (n x p) of predictors, where each row represents an observation.
#' @param Y A numeric vector (n) of response variable values.
#' @param mHat A numeric vector (n) of estimated mean values for each observation.
#' @return A numeric vector (n) of computed weights for each observation. Each weight is calculated as the
#'         ratio of the kernel-weighted sum of squared residuals to the kernel-weighted sum of kernel values.
#' @details The function uses the following formula to compute weights for each observation \(k\):
#' \deqn{
#'   \widehat{w}(\mathbf{x}_k) = \frac{\sum_{i=1}^n K_h(\mathbf{x}_i - \mathbf{x}_k) \cdot (Y_i - \widehat{m}(\mathbf{x}_i^\mathrm{T} \boldsymbol{\alpha}))^2}
#'   {\sum_{i=1}^n K_h(\mathbf{x}_i - \mathbf{x}_k)}
#' }
#' Here, \(K_h\) represents the kernel function, which is assumed to be Gaussian in the implementation.
#'
#' The computation is parallelized using OpenMP for improved performance on large datasets. Ensure your
#' environment supports OpenMP for optimal execution.
#'
#' @note The kernel function used is Gaussian (`gaussianKernel`) and the weights are normalized by dividing by their sum.
#' @export
cmptWeights <- function(X, Y, mHat) {
    .Call('_DMDR_cmptWeights', PACKAGE = 'DMDR', X, Y, mHat)
}

#' @name estimateScore
#' @title Estimate Scores and Parameters
#' @description Implements the core algorithm for estimating scores and model parameters based on regression and kernel density estimation.
#' @param X A numeric matrix (n x p) of predictors, where each row corresponds to an observation and each column represents a predictor variable.
#' @param alpha A numeric matrix (p x d) of transformation coefficients used to project the predictor variables into a lower-dimensional space. The transformation is applied to each row of X.
#' @param Y A numeric vector (n) of responses corresponding to each observation in X. This is the dependent variable in the regression.
#' @param h1 A numeric value representing the bandwidth parameter for the regression kernel. This parameter controls the smoothing effect in the regression step, affecting how much weight is given to nearby observations during the local linear fitting.
#' @param h2 A numeric value representing the bandwidth parameter for weight estimation kernel. This bandwidth is used in estimating the weighted averages of the responses, controlling the smoothness of the estimated weights.
#' @param h3 A numeric value representing the bandwidth parameter for covariate adjustment kernel. It affects the kernel used to estimate the weighted averages of the predictors, determining how much influence nearby predictor values have on the estimation process.
#' @return A list containing the following components:
#'   - \code{mHat}: A numeric vector of estimated intercepts from the local linear regression at each observation. This represents the estimated value of the response variable after transformation using the projection matrix \code{alpha}.
#'   - \code{m1Hat}: A numeric matrix of estimated slope coefficients. Each column corresponds to the slope coefficients associated with each predictor variable after projection, representing how each predictor affects the response variable in the transformed space.
#'   - \code{eWHat}: A numeric vector of weighted averages of responses, calculated using kernel density estimation. This is the conditional expectation of the responses, weighted by the kernel function based on the given bandwidth \code{h2}.
#'   - \code{exWHat}: A numeric matrix of weighted averages of the predictor variables, similarly estimated using kernel density functions for each predictor. It represents the conditional expectation of the predictors, weighted by the kernel function based on the bandwidth \code{h3}.
#'   - \code{scoreHat}: A numeric matrix of estimated scores for each observation. These scores are computed based on the weighted residuals from the local regression, adjusted by the estimated weights from \code{eWHat} and \code{exWHat}.
#'   - \code{eScore}: A numeric vector of mean scores, representing the average score across all observations, useful for model evaluation or assessment of overall fit.
#'   - \code{H}: A numeric matrix representing the Hessian of the model at each observation, used to estimate second-order partial derivatives of the loss function. This matrix is important for assessing the curvature of the model and is essential for optimization steps involving second-order methods.
#' @export
estimateScore <- function(X, alpha, Y, h1, h2, h3) {
    .Call('_DMDR_estimateScore', PACKAGE = 'DMDR', X, alpha, Y, h1, h2, h3)
}

#' @name estimateScoreSparse
#' @title Estimate Sparse Scores and Parameters
#' @description Implements the core algorithm for estimating scores and parameters with an emphasis on sparsity. This method incorporates sparsity-inducing techniques into the regression and kernel density estimation process.
#'
#' @param X A numeric matrix (n x p) of predictors, where each row corresponds to an observation and each column represents a predictor variable.
#' @param alpha A numeric matrix (p x d) of transformation coefficients used to project the predictor variables into a lower-dimensional space, incorporating sparsity constraints. This matrix ensures that many coefficients may be zero, enhancing interpretability.
#' @param Y A numeric vector (n) of responses corresponding to each observation in X. This is the dependent variable in the regression.
#' @param h1 A numeric value representing the bandwidth parameter for the regression kernel. Controls the smoothing effect in the sparse regression step, determining the weight given to nearby observations.
#' @param h2 A numeric value representing the bandwidth parameter for weight estimation kernel. Determines the smoothness of the estimated weights, incorporating sparsity in weighted averages of responses.
#' @param h3 A numeric value representing the bandwidth parameter for covariate adjustment kernel. Controls the smoothness of the kernel used to estimate weighted averages of the predictors, ensuring sparsity in the estimation process.
#'
#' @return A list containing the following components:
#'   - \code{mHat}: A numeric vector of estimated intercepts from the sparse local regression at each observation. Represents the response variable after applying the sparse transformation matrix \code{alpha}.
#'   - \code{m1Hat}: A numeric matrix of estimated sparse slope coefficients. Each column corresponds to the slope coefficients associated with each predictor after projection, ensuring sparsity in the representation.
#'   - \code{eWHat}: A numeric vector of weighted averages of responses, calculated using sparse kernel density estimation. Represents the conditional expectation of the responses, weighted by the kernel function with sparsity regularization.
#'   - \code{exWHat}: A numeric matrix of weighted averages of the predictor variables, estimated using sparse kernel density functions. Represents the conditional expectation of predictors, incorporating sparsity constraints.
#'   - \code{scoreHat}: A numeric matrix of sparse scores for each observation, computed based on the residuals from sparse local regression, adjusted by the sparse weights from \code{eWHat} and \code{exWHat}.
#'   - \code{eScore}: A numeric vector of mean sparse scores, representing the average sparse score across observations. Useful for assessing overall model sparsity and fit.
#'   - \code{H}: A numeric matrix representing the sparse Hessian of the model at each observation. Captures the second-order partial derivatives of the loss function with sparsity constraints, essential for optimization steps involving second-order methods.
#'
#' @details
#' The `estimateScoreSparse` function builds upon the `estimateScore` function by incorporating sparsity-inducing techniques. These techniques enforce zero coefficients in the transformation matrix \code{alpha} and ensure sparse representations in all estimation steps. The sparse solution is particularly beneficial for high-dimensional data where many predictors may be irrelevant. 
#'
#' Sparsity enhances model interpretability and reduces computational complexity while preserving essential information. The algorithm iteratively refines estimates using sparse kernels and ensures convergence through second-order optimization methods. The Hessian matrix is adjusted to account for sparsity in curvature assessments.
#'
#' @export
estimateScoreSparse <- function(X, alpha, Y, h1, h2, h3) {
    .Call('_DMDR_estimateScoreSparse', PACKAGE = 'DMDR', X, alpha, Y, h1, h2, h3)
}

#' @name projection
#' @title Compute Projection Matrix
#' @description Calculates the projection matrix for a given matrix \( B \), which represents the subspace defined by \( B \).
#'
#' @param B A numeric matrix (\( m \times n \)), representing the basis of a subspace.
#'
#' @return A numeric matrix (\( m \times m \)), which is the projection matrix \( P \):
#' \deqn{P = B (B^T B + \epsilon I)^{-1} B^T}
#' where \( \epsilon = 10^{-6} \) ensures numerical stability.
#'
#' @details
#' The projection matrix \( P \) is computed using a regularized inverse to ensure stability when \( B^T B \) is near-singular. 
#' This is useful for projecting vectors onto the subspace defined by the columns of \( B \).
#'
#' @examples
#' # Example in R
#' B <- matrix(rnorm(20), nrow = 5)
#' P <- projection(B)
#' print(P)
#'
#' @export
projection <- function(B) {
    .Call('_DMDR_projection', PACKAGE = 'DMDR', B)
}

#' @name trCor
#' @title Calculate Trace Correlation Between Subspaces
#' @description Computes the trace correlation between two subspaces defined by \( \beta \) and \( \beta_{\text{hat}} \), based on their projection matrices.
#'
#' @param beta A numeric matrix (\( m \times n \)), defining the first subspace.
#' @param beta_hat A numeric matrix (\( m \times p \)), defining the second subspace.
#'
#' @return A numeric scalar representing the trace correlation:
#' \deqn{\text{trCor} = \frac{\text{tr}(P_{\text{beta\_hat}} P_{\text{beta}})}{n}}
#' where \( P_{\text{beta}} \) and \( P_{\text{beta\_hat}} \) are the projection matrices for \( \beta \) and \( \beta_{\text{hat}} \), respectively, and \( n \) is the number of columns in \( \beta \).
#'
#' @details
#' Trace correlation measures the similarity between two subspaces by comparing their projection matrices. It ranges from 0 to 1, where higher values indicate greater similarity. The function internally uses the `projection` function to compute the required projection matrices.
#'
#' @examples
#' # Example in R
#' beta <- matrix(rnorm(20), nrow = 5)
#' beta_hat <- matrix(rnorm(15), nrow = 5)
#' trace_correlation <- trCor(beta, beta_hat)
#' print(trace_correlation)
#'
#' @export
trCor <- function(beta, beta_hat) {
    .Call('_DMDR_trCor', PACKAGE = 'DMDR', beta, beta_hat)
}

