% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{solveLasso}
\alias{solveLasso}
\title{Solve Lasso Regression Using Coordinate Descent}
\usage{
solveLasso(H1, betaCoef, lambda = 0.1, maxIter = 100L, tol = 1e-06)
}
\arguments{
\item{H1}{A numeric matrix (p x p) representing the covariance matrix or a related positive-definite matrix derived from the predictors. A small diagonal perturbation is added to ensure numerical stability.}

\item{betaCoef}{A numeric vector (p) representing the linear coefficients or gradient vector, typically derived from the regression problem.}

\item{lambda}{A numeric value specifying the regularization parameter. Higher values of \code{lambda} enforce greater sparsity by shrinking more coefficients to zero (default: 0.1).}

\item{maxIter}{An integer specifying the maximum number of iterations for the coordinate descent algorithm (default: 100).}

\item{tol}{A numeric value specifying the convergence tolerance. The algorithm terminates when the change in coefficients between iterations falls below this threshold (default: 1e-6).}
}
\value{
A numeric vector (p) of estimated regression coefficients. Sparse solutions are achieved due to the \(\ell_1\)-penalty applied by the Lasso method.
}
\description{
Implements the Lasso (Least Absolute Shrinkage and Selection Operator) regression using the coordinate descent algorithm. It solves for sparse regression coefficients by penalizing the \(\ell_1\)-norm of the coefficients.
}
\details{
The \code{solveLasso} function uses the coordinate descent algorithm to optimize the Lasso objective function:
\deqn{\frac{1}{2} \mathbf{x}^T H_1 \mathbf{x} + \mathbf{\beta}^T \mathbf{x} + \lambda \|\mathbf{x}\|_1,}
where \(\|\mathbf{x}\|_1\) is the \(\ell_1\)-norm of \(\mathbf{x}\), encouraging sparsity in the solution.

Each iteration updates one coefficient at a time while keeping others fixed. The update uses a soft-thresholding operator to shrink coefficients based on the penalty parameter \code{lambda}. The algorithm iteratively refines the coefficients until convergence is achieved or the maximum number of iterations is reached.

To ensure numerical stability, a small diagonal perturbation (e.g., \(10^{-3}\)) is added to \code{H1}, forming the matrix \code{H} internally. This helps prevent singularity or poorly conditioned matrices from affecting convergence.
}
\examples{
\dontrun{
H1 <- matrix(c(4, 1, 1, 3), nrow = 2)
betaCoef <- c(-1, 2)
lambda <- 0.5
result <- solveLasso(H1, betaCoef, lambda)
print(result)  # Sparse solution with \ell_1 regularization
}
}
